{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c13bab",
   "metadata": {},
   "source": [
    "# Costa Rican Household Poverty Level Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa20022",
   "metadata": {},
   "source": [
    "- 가구별 가난 등급...?을 예측\n",
    "  - 1: extreme poverty / 2: moderate poverty / 3: vulnerable households / non vulnerabel households\n",
    "- 각 가구원별로 칼럼이 있지만 prediction은 가장의 row에 대해서만 도출\n",
    "- 평가 지표: macro F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f086df",
   "metadata": {},
   "source": [
    "#### 변수 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb447d",
   "metadata": {},
   "source": [
    "- v2a1, Monthly rent payment  \n",
    "- hacdor, =1 Overcrowding by bedrooms\n",
    "- rooms,  number of all rooms in the house\n",
    "- hacapo, =1 Overcrowding by rooms\n",
    "- v14a, =1 has bathroom in the household\n",
    "- refrig, =1 if the household has refrigerator\n",
    "- v18q, owns a tablet\n",
    "- v18q1, number of tablets household owns\n",
    "- r4h1, Males younger than 12 years of age\n",
    "- r4h2, Males 12 years of age and older\n",
    "- r4h3, Total males in the household\n",
    "- r4m1, Females younger than 12 years of age\n",
    "- r4m2, Females 12 years of age and older\n",
    "- r4m3, Total females in the household\n",
    "- r4t1, persons younger than 12 years of age\n",
    "- r4t2, persons 12 years of age and older\n",
    "- r4t3, Total persons in the household\n",
    "- tamhog, size of the household\n",
    "- tamviv, number of persons living in the household\n",
    "- escolari, years of schooling\n",
    "- rez_esc, Years behind in school\n",
    "- hhsize, household size\n",
    "- paredblolad, =1 if predominant material on the outside wall is block or brick\n",
    "- paredzocalo, \"=1 if predominant material on the outside wall is socket (wood,  zinc or absbesto\"\n",
    "- paredpreb, =1 if predominant material on the outside wall is prefabricated or cement\n",
    "- pareddes, =1 if predominant material on the outside wall is waste material\n",
    "- paredmad, =1 if predominant material on the outside wall is wood\n",
    "- paredzinc, =1 if predominant material on the outside wall is zink\n",
    "- paredfibras, =1 if predominant material on the outside wall is natural fibers\n",
    "- paredother, =1 if predominant material on the outside wall is other\n",
    "- pisomoscer, \"=1 if predominant material on the floor is mosaic,  ceramic,  terrazo\"\n",
    "- pisocemento, =1 if predominant material on the floor is cement\n",
    "- pisoother, =1 if predominant material on the floor is other\n",
    "- pisonatur, =1 if predominant material on the floor is  natural material\n",
    "- pisonotiene, =1 if no floor at the household\n",
    "- pisomadera, =1 if predominant material on the floor is wood\n",
    "- techozinc, =1 if predominant material on the roof is metal foil or zink\n",
    "- techoentrepiso, \"=1 if predominant material on the roof is fiber cement,  mezzanine \"\n",
    "- techocane, =1 if predominant material on the roof is natural fibers\n",
    "- techootro, =1 if predominant material on the roof is other\n",
    "- cielorazo, =1 if the house has ceiling\n",
    "- abastaguadentro, =1 if water provision inside the dwelling\n",
    "- abastaguafuera, =1 if water provision outside the dwelling\n",
    "- abastaguano, =1 if no water provision\n",
    "- public, \"=1 electricity from CNFL,  ICE,  ESPH/JASEC\"\n",
    "- planpri, =1 electricity from private plant\n",
    "- noelec, =1 no electricity in the dwelling\n",
    "- coopele, =1 electricity from cooperative\n",
    "- sanitario1, =1 no toilet in the dwelling\n",
    "- sanitario2, =1 toilet connected to sewer or cesspool\n",
    "- sanitario3, =1 toilet connected to  septic tank\n",
    "- sanitario5, =1 toilet connected to black hole or letrine\n",
    "- sanitario6, =1 toilet connected to other system\n",
    "- energcocinar1, =1 no main source of energy used for cooking (no kitchen)\n",
    "- energcocinar2, =1 main source of energy used for cooking electricity\n",
    "- energcocinar3, =1 main source of energy used for cooking gas\n",
    "- energcocinar4, =1 main source of energy used for cooking wood charcoal\n",
    "- elimbasu1, =1 if rubbish disposal mainly by tanker truck\n",
    "- elimbasu2, =1 if rubbish disposal mainly by botan hollow or buried\n",
    "- elimbasu3, =1 if rubbish disposal mainly by burning\n",
    "- elimbasu4, =1 if rubbish disposal mainly by throwing in an unoccupied space\n",
    "- elimbasu5, \"=1 if rubbish disposal mainly by throwing in river,  creek or sea\"\n",
    "- elimbasu6, =1 if rubbish disposal mainly other\n",
    "- epared1, =1 if walls are bad\n",
    "- epared2, =1 if walls are regular\n",
    "- epared3, =1 if walls are good\n",
    "- etecho1, =1 if roof are bad\n",
    "- etecho2, =1 if roof are regular\n",
    "- etecho3, =1 if roof are good\n",
    "- eviv1, =1 if floor are bad\n",
    "- eviv2, =1 if floor are regular\n",
    "- eviv3, =1 if floor are good\n",
    "- dis, =1 if disable person\n",
    "- male, =1 if male\n",
    "- female, =1 if female\n",
    "- estadocivil1, =1 if less than 10 years old\n",
    "- estadocivil2, =1 if free or coupled uunion\n",
    "- estadocivil3, =1 if married\n",
    "- estadocivil4, =1 if divorced\n",
    "- estadocivil5, =1 if separated\n",
    "- estadocivil6, =1 if widow/er\n",
    "- estadocivil7, =1 if single\n",
    "- **parentesco1, =1 if household head**\n",
    "- parentesco2, =1 if spouse/partner\n",
    "- parentesco3, =1 if son/doughter\n",
    "- parentesco4, =1 if stepson/doughter\n",
    "- parentesco5, =1 if son/doughter in law\n",
    "- parentesco6, =1 if grandson/doughter\n",
    "- parentesco7, =1 if mother/father\n",
    "- parentesco8, =1 if father/mother in law\n",
    "- parentesco9, =1 if brother/sister\n",
    "- parentesco10, =1 if brother/sister in law\n",
    "- parentesco11, =1 if other family member\n",
    "- parentesco12, =1 if other non family member\n",
    "- **idhogar, Household level identifier**\n",
    "- hogar_nin, Number of children 0 to 19 in household\n",
    "- hogar_adul, Number of adults in household\n",
    "- hogar_mayor, # of individuals 65+ in the household\n",
    "- hogar_total, # of total individuals in the household\n",
    "- dependency, Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n",
    "- edjefe, years of education of male head of household, based on the interaction of escolari (years of education), head of household and \n",
    "- gender, yes=1 and no=0\n",
    "- edjefa, years of education of female head of household, based on the interaction of escolari (years of education), head of household and  - gender, yes=1 and no=0\n",
    "- meaneduc,average years of education for adults (18+)\n",
    "- instlevel1, =1 no level of education\n",
    "- instlevel2, =1 incomplete primary\n",
    "- instlevel3, =1 complete primary\n",
    "- instlevel4, =1 incomplete academic secondary level\n",
    "- instlevel5, =1 complete academic secondary level\n",
    "- instlevel6, =1 incomplete technical secondary level\n",
    "- instlevel7, =1 complete technical secondary level\n",
    "- instlevel8, =1 undergraduate and higher education\n",
    "- instlevel9, =1 postgraduate higher education\n",
    "- bedrooms, number of bedrooms\n",
    "- overcrowding, # persons per room\n",
    "- tipovivi1, =1 own and fully paid house\n",
    "- tipovivi2, \"=1 own,  paying in installments\"\n",
    "- tipovivi3, =1 rented\n",
    "- tipovivi4, =1 precarious\n",
    "- tipovivi5, \"=1 other(assigned,  borrowed)\"\n",
    "- computer, =1 if the household has notebook or desktop computer\n",
    "- television, =1 if the household has TV\n",
    "- mobilephone, =1 if mobile phone\n",
    "- qmobilephone, # of mobile phones\n",
    "- lugar1, =1 region Central\n",
    "- lugar2, =1 region Chorotega\n",
    "- lugar3, =1 region PacÃƒÂ­fico central\n",
    "- lugar4, =1 region Brunca\n",
    "- lugar5, =1 region Huetar AtlÃƒÂ¡ntica\n",
    "- lugar6, =1 region Huetar Norte\n",
    "- area1, =1 zona urbana\n",
    "- area2, =2 zona rural\n",
    "- age, Age in years\n",
    "- SQBescolari, escolari squared\n",
    "- SQBage, age squared\n",
    "- SQBhogar_total, hogar_total squared\n",
    "- SQBedjefe, edjefe squared\n",
    "- SQBhogar_nin, hogar_nin squared\n",
    "- SQBovercrowding, overcrowding squared\n",
    "- SQBdependency, dependency squared\n",
    "- SQBmeaned, square of the mean years of education of adults (>=18) in the household\n",
    "- agesq, Age squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a381e",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d52ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayoungcho/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1ac63",
   "metadata": {},
   "source": [
    "##  전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8dc7403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# target 변수 라벨인코딩\n",
    "def encode_data(df):\n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])  \n",
    "\n",
    "# 변수중요도 정렬\n",
    "def feature_importance(forest, X_train, display_results=True):\n",
    "    ranked_list = []\n",
    "    zero_features = []\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    if display_results:\n",
    "        print(\"Feature ranking:\")\n",
    "    for f in range(X_train.shape[1]):\n",
    "        if display_results:\n",
    "            print(\"%d. feature %d (%f)\" % (f+1, indices[f], importances[f]) + \"-\" + X_train.columns[indices[f]])\n",
    "        ranked_list.append(X_train.columns[indices[f]])\n",
    "\n",
    "        if importances[indices[f]] == 0.0:\n",
    "            zero_features.append(X_train.columns[indices[f]])\n",
    "\n",
    "    return ranked_list, zero_features  # 변수중요도 정렬한 리스트 / 변수중요도가 0인 변수들 리스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41161ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_features(df):   # 새 변수 생성\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'),             # 가구 내 인원 중 12살 미만 어린이의 비율\n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),          # 가구 내 인원 중 12살 초과 남성의 비율\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),              # 가구 내 인원 중 남자의 비율\n",
    "                 ('human_density', 'tamviv', 'rooms'),              # 가구 내 실거주인원 / 방의 개수\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),       # 가구 내 실거주인원 / 침실의 개수\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),               # 1인당 월세(월세 / 가구 내 인원)\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),                # 방 1개당 월세(월세 / 방의 개수)\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),        # 휴대폰 개수 / 가구 내 인원\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),               # 태블릿 개수 / 가구 내 인원\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),  # 휴대폰 개수 / 12세 이상 인원\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),         # 태블릿 개수 / 12세 이상 인원\n",
    "                ]\n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),         # 가구 내 인원 - 가구 내 실거주인원\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "    \n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_'+f_new] = (df[f1] / df[f2]).astype(np.float32)\n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_'+f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "\n",
    "    aggs_num = {'age' : ['min', 'max', 'mean'],\n",
    "                'escolari': ['min', 'max', 'mean']}  # escolari: years of schooling\n",
    "    aggs_cat = {'dis': ['mean']}                     # dis: 장애 여부(있으면 1) -> 이진 변수의 평균: 비율\n",
    "\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:           \n",
    "        # estadocivil: 10세 미만 / 커플? / 기혼 / 이혼 / 별거 / 미망인 / 솔로  - 7개의 상태를 원핫인코딩으로 7개의 변수로\n",
    "        # parentesco: 가장 본인 / 아내 혹은 파트너 / 자식 / 의붓아들 혹은 딸 / 사위 혹은 며느리 / 증손자 혹은 증손녀 / 어머니 혹은 아버지 / 시어머니 혹은 시아버지 / 형제 / 처형 처남 등등.. / 그 외 가족 / 가족 외 - 12개의 가장과의 관계를 원핫인코딩으로 12개의 변수로 나눔\n",
    "        # instlevel: 교육X / 초등학교 중퇴 / 초졸 / 중학교 중퇴 / 중졸 / 고등학교 중퇴 / 고졸 / 대학교 재학 / 대졸 - 9개의 상태\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean', 'count']\n",
    "\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '-' + e[0] + '-' + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0d45b",
   "metadata": {},
   "source": [
    "📌 결과적으로 생성되는 피처 예시\n",
    "- agg18-age-MIN: 해당 가구 내 18세 이상 구성원의 최솟값 나이\n",
    "\n",
    "- agg18-dis-MEAN: 해당 가구 내 18세 이상 중 장애인 비율\n",
    "\n",
    "- agg18-parentesco_1-MEAN: 가구주 본인 비율\n",
    "\n",
    "- agg18-instlevel_8-COUNT: 대학교 재학 중인 구성원 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a00b3a",
   "metadata": {},
   "source": [
    "r4t3랑 tamviv랑 tamhog랑 뭐가 다른지 모르곘음  \n",
    "- r4t3: total persons in the household\n",
    "- tamviv: number of persons living in the household\n",
    "- tamhog: size of the household"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6d8c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>r4t3</th>\n",
       "      <th>tamviv</th>\n",
       "      <th>tamhog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9537</th>\n",
       "      <td>ID_cca751e53</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9538</th>\n",
       "      <td>ID_bfbb06d62</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>ID_e20c78904</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9540</th>\n",
       "      <td>ID_e01b00c7e</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9541</th>\n",
       "      <td>ID_a31274054</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>ID_fc386a944</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9543</th>\n",
       "      <td>ID_2f8268634</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>ID_4c180d79f</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>ID_32a00a8bf</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9546</th>\n",
       "      <td>ID_1dfb12fcf</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9547</th>\n",
       "      <td>ID_198be48d1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9548</th>\n",
       "      <td>ID_9df63c33e</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9549</th>\n",
       "      <td>ID_aacac04a2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9550</th>\n",
       "      <td>ID_90a399a51</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9551</th>\n",
       "      <td>ID_79d39dddc</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9552</th>\n",
       "      <td>ID_d45ae367d</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9553</th>\n",
       "      <td>ID_c94744e07</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9554</th>\n",
       "      <td>ID_85fc658f8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9555</th>\n",
       "      <td>ID_ced540c61</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9556</th>\n",
       "      <td>ID_a38c64491</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id  r4t3  tamviv  tamhog\n",
       "9537  ID_cca751e53     5       5       5\n",
       "9538  ID_bfbb06d62     5       5       5\n",
       "9539  ID_e20c78904     5       5       5\n",
       "9540  ID_e01b00c7e     5       5       5\n",
       "9541  ID_a31274054     5       5       5\n",
       "9542  ID_fc386a944     5       5       5\n",
       "9543  ID_2f8268634     5       5       5\n",
       "9544  ID_4c180d79f     5       5       5\n",
       "9545  ID_32a00a8bf     5       7       5\n",
       "9546  ID_1dfb12fcf     5       7       5\n",
       "9547  ID_198be48d1     5       7       5\n",
       "9548  ID_9df63c33e     5       7       5\n",
       "9549  ID_aacac04a2     5       7       5\n",
       "9550  ID_90a399a51     2       2       2\n",
       "9551  ID_79d39dddc     2       2       2\n",
       "9552  ID_d45ae367d     5       5       5\n",
       "9553  ID_c94744e07     5       5       5\n",
       "9554  ID_85fc658f8     5       5       5\n",
       "9555  ID_ced540c61     5       5       5\n",
       "9556  ID_a38c64491     5       5       5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/Users/dayoungcho/Desktop/costa-rican-household-poverty-prediction/train.csv')\n",
    "train_df[['Id', 'r4t3', 'tamviv', 'tamhog']].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25974806",
   "metadata": {},
   "source": [
    "출력해봐도 모르겠음..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19d5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫인코딩된 변수들을 라벨인코딩한 하나의 변수로 통합\n",
    "\n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', 'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', 'instlevel', 'lugar', 'tipovivi','manual_elec']:\n",
    "        if 'manual_' not in s_:  # manual_elec 제외한 변수들\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']   # 전기의 출처(발전소/기업 등등..)\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "\n",
    "        if 0 in sum_ohe:\n",
    "            print('{} 안의 OHE가 불완전하므로 라벨 인코딩 전에 새로운 칼럼을 추가합니다'.format(s_))\n",
    "        \n",
    "            col_dummy = s_ + '_dummy'\n",
    "            tmp_df[col_dummy] = s_ + '_dummy'\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1)==0).astype(np.int8)\n",
    "            cols_s_.append(col_dummy)\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                print(\"제대로 안 됨..\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)  # 최대값의 인덱스 반환\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c746fd2",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fba659",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/dayoungcho/Desktop/costa-rican-household-poverty-prediction/train.csv')\n",
    "test = pd.read_csv('/Users/dayoungcho/Desktop/costa-rican-household-poverty-prediction/test.csv')\n",
    "\n",
    "test_ids = test.Id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcae55",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0a16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    encode_data(df_)  # target 변수 라벨인코딩\n",
    "    return do_features(df_)  # 새로운 변수들 생성\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cbc5608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>r4h2</th>\n",
       "      <th>...</th>\n",
       "      <th>agg18-instlevel5-MEAN</th>\n",
       "      <th>agg18-instlevel5-COUNT</th>\n",
       "      <th>agg18-instlevel6-MEAN</th>\n",
       "      <th>agg18-instlevel6-COUNT</th>\n",
       "      <th>agg18-instlevel7-MEAN</th>\n",
       "      <th>agg18-instlevel7-COUNT</th>\n",
       "      <th>agg18-instlevel8-MEAN</th>\n",
       "      <th>agg18-instlevel8-COUNT</th>\n",
       "      <th>agg18-instlevel9-MEAN</th>\n",
       "      <th>agg18-instlevel9-COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 218 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  r4h1  r4h2  \\\n",
       "0  190000.0       0      3       0     1       1     0    NaN     0     1   \n",
       "1  135000.0       0      4       0     1       1     1    1.0     0     1   \n",
       "2       NaN       0      8       0     1       1     0    NaN     0     0   \n",
       "3  180000.0       0      5       0     1       1     1    1.0     0     2   \n",
       "4  180000.0       0      5       0     1       1     1    1.0     0     2   \n",
       "\n",
       "   ...  agg18-instlevel5-MEAN  agg18-instlevel5-COUNT  agg18-instlevel6-MEAN  \\\n",
       "0  ...                    0.0                     1.0                    0.0   \n",
       "1  ...                    0.0                     1.0                    0.0   \n",
       "2  ...                    1.0                     1.0                    0.0   \n",
       "3  ...                    1.0                     2.0                    0.0   \n",
       "4  ...                    1.0                     2.0                    0.0   \n",
       "\n",
       "   agg18-instlevel6-COUNT  agg18-instlevel7-MEAN  agg18-instlevel7-COUNT  \\\n",
       "0                     1.0                    0.0                     1.0   \n",
       "1                     1.0                    0.0                     1.0   \n",
       "2                     1.0                    0.0                     1.0   \n",
       "3                     2.0                    0.0                     2.0   \n",
       "4                     2.0                    0.0                     2.0   \n",
       "\n",
       "   agg18-instlevel8-MEAN  agg18-instlevel8-COUNT  agg18-instlevel9-MEAN  \\\n",
       "0                    0.0                     1.0                    0.0   \n",
       "1                    1.0                     1.0                    0.0   \n",
       "2                    0.0                     1.0                    0.0   \n",
       "3                    0.0                     2.0                    0.0   \n",
       "4                    0.0                     2.0                    0.0   \n",
       "\n",
       "   agg18-instlevel9-COUNT  \n",
       "0                     1.0  \n",
       "1                     1.0  \n",
       "2                     1.0  \n",
       "3                     2.0  \n",
       "4                     2.0  \n",
       "\n",
       "[5 rows x 218 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b40b0805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v2a1                      6860\n",
       "hacdor                       0\n",
       "rooms                        0\n",
       "hacapo                       0\n",
       "v14a                         0\n",
       "                          ... \n",
       "agg18-instlevel7-COUNT      15\n",
       "agg18-instlevel8-MEAN       15\n",
       "agg18-instlevel8-COUNT      15\n",
       "agg18-instlevel9-MEAN       15\n",
       "agg18-instlevel9-COUNT      15\n",
       "Length: 218, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()   # 결측치 있는 row 여러 개 존재.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39594fcf",
   "metadata": {},
   "source": [
    "### 결측치 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62e659a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그냥 dependency랑 dependency의 제곱 두 개의 변수가 따로 존재함..(왜..?) -> dependeny의 결측치를 SQBdependency의 루트로 대체\n",
    "train['dependency'] = np.sqrt(train['SQBdependency'])  \n",
    "test['dependency'] = np.sqrt(test['SQBdependency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23d8272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# education에서 'no'를 0으로 대체\n",
    "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0  # edjefa: 여성 가장의 교육받은 년수\n",
    "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0  # edjefe: 님상 가장의 교육받은 년수\n",
    "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
    "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
    "\n",
    "# education이 'yes'이고 그 사람이 가장일 경우 escolari(교육받은 년수)로 대체\n",
    "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
    "\n",
    "# education='yes'인데 그 사람이 가장이 아닌 경우 그냥 4로 대체...\n",
    "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
    "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
    "\n",
    "# int타입으로 변경\n",
    "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
    "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
    "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
    "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
    "\n",
    "# edjefa와 edjefe 중 큰 값 변수 생성(가장의 성별과 상관 없이 그냥 총 교육 년수)\n",
    "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
    "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "180c37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2a1, v18q1, rez_esc, meanduc, SQBmeaned는 결측치를 0으로 대체\n",
    "train['v2a1']=train['v2a1'].fillna(0)\n",
    "test['v2a1']=test['v2a1'].fillna(0)\n",
    "\n",
    "test['v18q1']=test['v18q1'].fillna(0)\n",
    "train['v18q1']=train['v18q1'].fillna(0)\n",
    "\n",
    "train['rez_esc']=train['rez_esc'].fillna(0)\n",
    "test['rez_esc']=test['rez_esc'].fillna(0)\n",
    "\n",
    "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
    "\n",
    "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
    "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81c8b0",
   "metadata": {},
   "source": [
    "### 변수 충돌 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8760d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v14a & sanitario1:화장실 존재 여부(v14a는 있으면 1, sanitario1은 없으면 1) -> 충돌이 있는 변수가 있음\n",
    "# abastaguano: 물 공급 여부(없을시 1) 변수를 이용하여 충돌 해결\n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0 \n",
    "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
    "\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
    "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a965050",
   "metadata": {},
   "source": [
    "### 인코딩 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69613a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "techo 안의 OHE가 불완전하므로 라벨 인코딩 전에 새로운 칼럼을 추가합니다\n",
      "instlevel 안의 OHE가 불완전하므로 라벨 인코딩 전에 새로운 칼럼을 추가합니다\n",
      "manual_elec 안의 OHE가 불완전하므로 라벨 인코딩 전에 새로운 칼럼을 추가합니다\n"
     ]
    }
   ],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_\n",
    "\n",
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09af8be",
   "metadata": {},
   "source": [
    "### geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6983fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역별 변수를 추가(지역별 바닥/천장/벽/쓰레기 수거 현황 등..)\n",
    "\n",
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']  # 범주형 변수\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']   # 등간/비율 척도(숫자형)\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe],   # 이산형->범주형 변환한걸 다시 더미화...(?)\n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)  # 더미변수의 평균 => 비율 나타냄\n",
    "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "782d1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18세 이상 성인 변수 추가\n",
    "\n",
    "# train['num_over_18'] = 0\n",
    "# train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "# train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "# train['num_over_18'] = train['num_over_18'].fillna(0)\n",
    "\n",
    "train['num_over_18'] = train['idhogar'].map(train[train['age'] >= 18].groupby('idhogar')['age'].count()).fillna(0).astype(int)\n",
    "\n",
    "# test['num_over_18'] = 0\n",
    "# test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
    "# test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
    "# test['num_over_18'] = test['num_over_18'].fillna(0)\n",
    "\n",
    "test['num_over_18'] = test['idhogar'].map(train[train['age']>=18].groupby('idhogar')['age'].count()).fillna(0).astype(int)\n",
    "# 기타 변수 추가...\n",
    "def extract_features(df):\n",
    "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms']\n",
    "    df['rent_to_rooms'] = df['v2a1']/df['rooms']\n",
    "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # tamhog - size of the household\n",
    "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - Total persons in the household\n",
    "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - Total persons in the household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # rent to people in household\n",
    "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # rent to people under age 12\n",
    "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # rooms per person\n",
    "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # rent to household size\n",
    "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18']\n",
    "    # some households have no one over 18, use the total rent for those\n",
    "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
    "    \n",
    "extract_features(train)    \n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df886aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쓸모없는 변수 버리기\n",
    "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
    "                 'mobilephone', 'female', ]\n",
    "\n",
    "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s]\n",
    "\n",
    "needless_cols.extend(instlevel_cols)\n",
    "\n",
    "train = train.drop(needless_cols, axis=1)\n",
    "test = test.drop(needless_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824597b",
   "metadata": {},
   "source": [
    "## data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fb4bb",
   "metadata": {},
   "source": [
    "가구 단위로 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6be3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.2, seed=None):\n",
    "    train2 = train.copy()\n",
    "\n",
    "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)  # 20% 가구 추출\n",
    "    cv_idx = np.isin(households, cv_hhs)\n",
    "    X_test = train2[cv_idx]\n",
    "    y_test = y[cv_idx]\n",
    "\n",
    "    X_train = train2[~cv_idx]\n",
    "    y_train = y[~cv_idx]\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        y_train_weights = sample_weight[~cv_idx]\n",
    "        return X_train, y_train, X_test, y_test, y_train_weights\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "435b8d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1       4\n",
       "2       4\n",
       "3       4\n",
       "4       4\n",
       "       ..\n",
       "9552    2\n",
       "9553    2\n",
       "9554    2\n",
       "9555    2\n",
       "9556    2\n",
       "Name: Target, Length: 9557, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10bf1566",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=None)\n",
    "\n",
    "X = train.copy()\n",
    "X = train.query('parentesco1==1')   # 가장 데이터만 추출\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)\n",
    "\n",
    "train2 = X.copy()\n",
    "train_hhs = train2.idhogar\n",
    "households = train2.idhogar.unique()\n",
    "cv_hhs = np.random.choice(households, size=int(len(households)*0.15),replace=False)   # 가구 중에서 랜덤하게 15% 선택\n",
    "cv_idx = np.isin(train2.idhogar, cv_hhs)\n",
    "\n",
    "X_test = train2[cv_idx] \n",
    "y_test = y[cv_idx]\n",
    "\n",
    "X_train = train2[~cv_idx]\n",
    "y_train = y[~cv_idx]\n",
    "\n",
    "X_train = train2  # 걍 이렇게 쓸거면 위에 코드는 왜 쓴거지?\n",
    "y_train = y\n",
    "\n",
    "train_households = X_train.idhogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f52daec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6892df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쓸데없는 변수 또 드랍\n",
    "\n",
    "extra_drop_features = [\n",
    " 'agg18_estadocivil1_MEAN',\n",
    " 'agg18_estadocivil6_COUNT',\n",
    " 'agg18_estadocivil7_COUNT',\n",
    " 'agg18_parentesco10_COUNT',\n",
    " 'agg18_parentesco11_COUNT',\n",
    " 'agg18_parentesco12_COUNT',\n",
    " 'agg18_parentesco1_COUNT',\n",
    " 'agg18_parentesco2_COUNT',\n",
    " 'agg18_parentesco3_COUNT',\n",
    " 'agg18_parentesco4_COUNT',\n",
    " 'agg18_parentesco5_COUNT',\n",
    " 'agg18_parentesco6_COUNT',\n",
    " 'agg18_parentesco7_COUNT',\n",
    " 'agg18_parentesco8_COUNT',\n",
    " 'agg18_parentesco9_COUNT',\n",
    " 'geo_elimbasu_LE_4',\n",
    " 'geo_energcocinar_LE_1',\n",
    " 'geo_energcocinar_LE_2',\n",
    " 'geo_epared_LE_0',\n",
    " 'geo_hogar_mayor',\n",
    " 'geo_manual_elec_LE_2',\n",
    " 'geo_pared_LE_3',\n",
    " 'geo_pared_LE_4',\n",
    " 'geo_pared_LE_5',\n",
    " 'geo_pared_LE_6',\n",
    " 'num_over_18',\n",
    " 'parentesco_LE',\n",
    " 'rez_esc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b91e1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_drop_cols = extra_drop_features + ['idhogar', 'parentesco1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc95b4",
   "metadata": {},
   "source": [
    "## Fit a voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70468681",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
    "\n",
    "def evaluate_macroF1_lgb(predictions, truth):\n",
    "    pred_labels = predictions.argmax(axis=1)\n",
    "    truth = truth.get_label()\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', 1-f1)\n",
    "\n",
    "fit_params = {'early_stopping_rounds': 500,\n",
    "              'eval_metric': evaluate_macroF1_lgb,\n",
    "              'eval_set': [(X_train, y_train), (X_test, y_test)],\n",
    "              'verbose': False}\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate * np.power(0.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['verbose'] = 50   # 왜 자꾸 뒤에서 따로or다시 정의하는지 모르겠음.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5257d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold = True, **fit_params):\n",
    "    estimator = clone(estimator1)\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households = train_households)\n",
    "    else:\n",
    "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
    "\n",
    "    fit_params['eval_set'] = [(X_test, y_test)]\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
    "    else:\n",
    "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
    "            estimator.fit(X_train, y_train)\n",
    "        else:\n",
    "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
    "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
    "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
    "    else:\n",
    "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
    "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
    "        print(\"Train F1:\", best_train)\n",
    "        print(\"Test F1:\", best_cv)\n",
    "        \n",
    "    # reject some estimators based on their performance on train and test sets\n",
    "    if threshold:\n",
    "        # if the valid score is very high we'll allow a little more leeway with the train scores\n",
    "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
    "            return estimator\n",
    "\n",
    "        # else recurse until we get a better one\n",
    "        else:\n",
    "            print(\"Unacceptable!!! Trying again...\")\n",
    "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
    "    \n",
    "    else:\n",
    "        return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81495aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    '''\n",
    "    This implements the fit method of the VotingClassifier propagating fit_params\n",
    "    '''\n",
    "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e081c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:34:03] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_b2k4a2hea3/croot/xgboost-split_1675457783214/work/src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(clfs)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Train the final model with learning rate decay\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mvc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_drop_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m clf_final \u001b[38;5;241m=\u001b[39m vc\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[29], line 40\u001b[0m, in \u001b[0;36mVotingClassifierLGBM.fit\u001b[0;34m(self, X, y, sample_weight, threshold, **fit_params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclfs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "Cell \u001b[0;32mIn[28], line 17\u001b[0m, in \u001b[0;36m_parallel_fit_estimator\u001b[0;34m(estimator1, X, y, sample_weight, threshold, **fit_params)\u001b[0m\n\u001b[1;32m     15\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator1, ExtraTreesClassifier) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator1, RandomForestClassifier):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/training.py:186\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    189\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/callback.py:240\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m name\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset name should not contain `-`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 240\u001b[0m score: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m splited \u001b[38;5;241m=\u001b[39m score\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# into datasets\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# split up `test-error:0.1234`\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:2002\u001b[0m, in \u001b[0;36mBooster.eval_set\u001b[0;34m(self, evals, iteration, feval, output_margin)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dmat, evname \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[0;32m-> 2002\u001b[0m         feval_ret \u001b[38;5;241m=\u001b[39m \u001b[43mfeval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdmat\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2005\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2006\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n\u001b[1;32m   2007\u001b[0m                 \u001b[38;5;66;03m# pylint: disable=consider-using-f-string\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mevaluate_macroF1_lgb\u001b[0;34m(predictions, truth)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_macroF1_lgb\u001b[39m(predictions, truth):\n\u001b[0;32m----> 4\u001b[0m     pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     truth \u001b[38;5;241m=\u001b[39m truth\u001b[38;5;241m.\u001b[39mget_label()\n\u001b[1;32m      6\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m f1_score(truth, pred_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "for i in range(15):\n",
    "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
    "    \n",
    "    clfs.append(('xgb{}'.format(i), clf))\n",
    "    \n",
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "del(clfs)\n",
    "\n",
    "#Train the final model with learning rate decay\n",
    "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1, errors='ignore'), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
    "\n",
    "clf_final = vc.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542d993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
